{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1f4c5c",
   "metadata": {},
   "source": [
    "# LunarLander with DQN (Deep Q-Network)\n",
    "\n",
    "In this lesson, you'll train a DQN agent on `LunarLander-v3` using Stable-Baselines3. We'll:\n",
    "- Understand the environment\n",
    "- Define a DQN model and key hyperparameters\n",
    "- Train with evaluation callbacks\n",
    "- Visualize learning curves and watch the agent land!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aa24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"Libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d91a30",
   "metadata": {},
   "source": [
    "## Environment and Wrappers\n",
    "We'll wrap the environment with `Monitor` to log episode rewards/lengths and set up an evaluation environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Monitor(gym.make(\"LunarLander-v3\"))\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v3\"))\n",
    "log_dir = \"./logs_dqn\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "print(env.observation_space, env.action_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1e58cf",
   "metadata": {},
   "source": [
    "## Define the DQN Model\n",
    "Key hyperparameters: network architecture, buffer size, learning rate, gamma, exploration schedule, and target network updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf7b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=2.5e-4,\n",
    "    buffer_size=100_000,\n",
    "    learning_starts=10_000,\n",
    "    batch_size=64,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=10_000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_initial_eps=1.0,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=log_dir,\n",
    "    log_path=log_dir,\n",
    "    eval_freq=10_000,\n",
    "    deterministic=True,\n",
    "    render=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae92bfbc",
   "metadata": {},
   "source": [
    "## Train and Evaluate\n",
    "We'll train for 200k steps. Evaluation runs periodically via the callback, and we compute final mean reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14dc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=200_000, callback=callback)\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=20)\n",
    "print(f\"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb53f9",
   "metadata": {},
   "source": [
    "# Lunar Lander with Gymnasium and Deep Q-Learning (DQN)\n",
    "\n",
    "DQN is an off-policy reinforcement learning algorithm that learns an action-value function $Q(s,a)$, which estimates the expected future reward of taking action a in state s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888408f",
   "metadata": {},
   "source": [
    "## Create the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14db406",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeff109",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DQN model\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",         # Use a Multi-Layer Perceptron (MLP) neural network\n",
    "    env=env,                    # The Gym environment\n",
    "    \n",
    "    # === Learning Parameters ===\n",
    "    learning_rate=1e-3,         # Step size for optimizer (Adam by default). Higher = faster but riskier updates.\n",
    "\n",
    "    # === Replay Buffer ===\n",
    "    buffer_size=50_000,         # Max number of past transitions to store. Larger = more stable learning but more memory.\n",
    "    learning_starts=1000,       # Number of steps before learning starts (helps fill the buffer with diverse experience).\n",
    "    batch_size=64,              # Number of samples per training update from the buffer.\n",
    "\n",
    "    # === Discounting and Target Network ===\n",
    "    gamma=0.99,                 # Discount factor for future rewards. Close to 1 = long-term reward focus.\n",
    "    tau=1.0,                    # Soft update rate for the target network. 1.0 = hard update every target_update_interval.\n",
    "\n",
    "    # === Training Frequency ===\n",
    "    train_freq=4,               # Train the model every 4 environment steps.\n",
    "    target_update_interval=1000,  # Number of training steps between target network updates (delayed update stabilizes learning).\n",
    "\n",
    "    # === Misc Settings ===\n",
    "    verbose=1,                  # Verbosity level: 0 = silent, 1 = training info, 2 = debug.\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=200_000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"dqn_lunarlander\")\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63355818",
   "metadata": {},
   "source": [
    "## Interpretting Training Log\n",
    "\n",
    "| Metric                        | Meaning                                                                                                                                                                                           |\n",
    "| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`ep_len_mean`**       | Average number of steps per episode. Max is 1000 for `LunarLander-v3`, so a high number means your agent usually survives (or crashes) near the end of each episode.                                        |\n",
    "| **`ep_rew_mean`**      | Average reward per episode over recent rollouts. In `LunarLander`, good agents score **\\~200**, random ones score near **0 or below**. A score below 200 indicates your agent hasn't converged yet. |\n",
    "| **`exploration_rate`** = 0.05 | Current ε in ε-greedy exploration. Starts near 1 (more random) and decays to min (usually 0.05). You’re now mostly exploiting learned policy.                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8aadd7",
   "metadata": {},
   "source": [
    "## Render the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329acabc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = DQN.load(\"dqn_lunarlander\")\n",
    "\n",
    "# Create environment with human rendering\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3696d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
