{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e05cfdc",
   "metadata": {},
   "source": [
    "# Blackjack with Policy Gradient (REINFORCE)\n",
    "\n",
    "We'll implement a simple policy-gradient agent for the Blackjack environment using Gymnasium's `Blackjack-v1`.\n",
    "\n",
    "Steps:\n",
    "- Define a soft policy with a small neural network\n",
    "- Sample episodes to estimate returns\n",
    "- Update policy with REINFORCE\n",
    "- Track performance over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606cdd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "print(\"Libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162416d6",
   "metadata": {},
   "source": [
    "## Environment & Policy Network\n",
    "State is `(player_sum, dealer_card, usable_ace)`. We'll one-hot encode discretized features and use a small MLP for Ï€(a|s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64295149",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", sab=True)\n",
    "\n",
    "# feature encoding: player 0..31, dealer 1..10, usable_ace {0,1}\n",
    "obs_space = (32, 11, 2)\n",
    "feat_dim = np.prod(obs_space)\n",
    "\n",
    "def encode(obs):\n",
    "    ps, dc, ua = obs\n",
    "    x = np.zeros(obs_space, dtype=np.float32)\n",
    "    x[min(ps, 31), dc, int(ua)] = 1.0\n",
    "    return x.reshape(-1)\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, input_dim, hidden=64, outputs=2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, outputs)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "policy = Policy(feat_dim)\n",
    "opt = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "print(\"Policy ready; input_dim=\", feat_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437cb262",
   "metadata": {},
   "source": [
    "## REINFORCE Loop\n",
    "Sample episodes, compute returns G_t, and update the policy to maximize expected return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614c4f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose(pi, obs):\n",
    "    x = torch.from_numpy(encode(obs)).float().unsqueeze(0)\n",
    "    logits = pi(x)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    dist = torch.distributions.Categorical(probs)\n",
    "    a = dist.sample()\n",
    "    return a.item(), dist.log_prob(a)\n",
    "\n",
    "returns, lengths = [], []\n",
    "for ep in range(2000):\n",
    "    obs, _ = env.reset()\n",
    "    logps, rs = [], []\n",
    "    done = False\n",
    "    while not done:\n",
    "        a, logp = choose(policy, obs)\n",
    "        obs, r, term, trunc, _ = env.step(a)\n",
    "        logps.append(logp)\n",
    "        rs.append(r)\n",
    "        done = term or trunc\n",
    "    # compute returns\n",
    "    G = 0.0\n",
    "    rets = []\n",
    "    for r in reversed(rs):\n",
    "        G = r + 0.99 * G\n",
    "        rets.append(G)\n",
    "    rets = list(reversed(rets))\n",
    "    returns.append(sum(rs))\n",
    "    lengths.append(len(rs))\n",
    "    # policy gradient step\n",
    "    opt.zero_grad()\n",
    "    loss = 0\n",
    "    for logp, Gt in zip(logps, rets):\n",
    "        loss += -logp * Gt\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if (ep+1) % 100 == 0:\n",
    "        print(f\"Episode {ep+1}: avg return (100) = {np.mean(returns[-100:]):.2f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb53f9",
   "metadata": {},
   "source": [
    "# Blackjack with Gymnasium and Deep Q-Learning (DQN)\n",
    "\n",
    "DQN is an off-policy reinforcement learning algorithm that learns an action-value function $Q(s,a)$, which estimates the expected future reward of taking action a in state s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888408f",
   "metadata": {},
   "source": [
    "## Create the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14db406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7eeff109",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b2720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac8aadd7",
   "metadata": {},
   "source": [
    "## Render the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329acabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3696d1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
