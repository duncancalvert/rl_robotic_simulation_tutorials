{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dynamic Programming on FrozenLake (Policy Evaluation & Iteration)\n",
        "\n",
        "Learn how to solve an MDP exactly using Dynamic Programming (DP):\n",
        "- Policy Evaluation\n",
        "- Policy Improvement\n",
        "- Policy Iteration\n",
        "- Value Iteration\n",
        "\n",
        "Environment: `FrozenLake-v1` (map_size=8, is_slippery=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True)\n",
        "S = env.observation_space.n\n",
        "A = env.action_space.n\n",
        "P = env.unwrapped.P  # transition dynamics dict\n",
        "print(\"States:\", S, \"Actions:\", A)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy Evaluation\n",
        "Given a policy π, compute V^π by iteratively applying the Bellman expectation backup until convergence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_evaluation(policy, P, gamma=0.99, tol=1e-8):\n",
        "    V = np.zeros(S)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(S):\n",
        "            v = 0\n",
        "            a = policy[s]\n",
        "            for prob, s2, r, done in P[s][a]:\n",
        "                v += prob * (r + gamma * V[s2])\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "            V[s] = v\n",
        "        if delta < tol:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "# random policy to start\n",
        "policy = np.random.randint(0, A, size=S)\n",
        "V = policy_evaluation(policy, P)\n",
        "V[:10]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy Improvement\n",
        "Given V^π, greedify the policy with respect to the action-value estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_improvement(V, P, gamma=0.99):\n",
        "    policy = np.zeros(S, dtype=int)\n",
        "    for s in range(S):\n",
        "        q = np.zeros(A)\n",
        "        for a in range(A):\n",
        "            for prob, s2, r, done in P[s][a]:\n",
        "                q[a] += prob * (r + gamma * V[s2])\n",
        "        policy[s] = int(np.argmax(q))\n",
        "    return policy\n",
        "\n",
        "new_policy = policy_improvement(V, P)\n",
        "new_policy[:20]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Policy Iteration\n",
        "Alternate policy evaluation and improvement until the policy stabilizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def policy_iteration(P, gamma=0.99):\n",
        "    policy = np.random.randint(0, A, size=S)\n",
        "    while True:\n",
        "        V = policy_evaluation(policy, P, gamma)\n",
        "        new_policy = policy_improvement(V, P, gamma)\n",
        "        if np.all(policy == new_policy):\n",
        "            return policy, V\n",
        "        policy = new_policy\n",
        "\n",
        "pi, V_pi = policy_iteration(P)\n",
        "print(\"Policy iteration done. Sample V:\", V_pi[:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Value Iteration\n",
        "Directly iterate on V using the Bellman optimality operator until convergence, then extract a greedy policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def value_iteration(P, gamma=0.99, tol=1e-8):\n",
        "    V = np.zeros(S)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(S):\n",
        "            q = np.zeros(A)\n",
        "            for a in range(A):\n",
        "                for prob, s2, r, done in P[s][a]:\n",
        "                    q[a] += prob * (r + gamma * V[s2])\n",
        "            v_new = np.max(q)\n",
        "            delta = max(delta, abs(v_new - V[s]))\n",
        "            V[s] = v_new\n",
        "        if delta < tol:\n",
        "            break\n",
        "    # greedy policy\n",
        "    pi = np.zeros(S, dtype=int)\n",
        "    for s in range(S):\n",
        "        q = np.zeros(A)\n",
        "        for a in range(A):\n",
        "            for prob, s2, r, done in P[s][a]:\n",
        "                q[a] += prob * (r + gamma * V[s2])\n",
        "        pi[s] = int(np.argmax(q))\n",
        "    return pi, V\n",
        "\n",
        "pi_opt, V_opt = value_iteration(P)\n",
        "print(\"Value iteration done. Sample V:\", V_opt[:10])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
