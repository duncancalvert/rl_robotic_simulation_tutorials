{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a279ea4",
   "metadata": {},
   "source": [
    "# CartPole with Q-Learning\n",
    "\n",
    "Welcome! In this interactive lesson, you'll implement Q-learning from scratch to solve `CartPole-v1`.\n",
    "\n",
    "What you'll do:\n",
    "- Understand the environment (state, action, reward)\n",
    "- Discretize continuous states into bins\n",
    "- Implement the Q-learning update rule\n",
    "- Train and evaluate the agent\n",
    "- Visualize training progress\n",
    "\n",
    "Tip: Run cells in order and read the explanation before each code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0066a7b",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We'll use Gymnasium for the environment, NumPy for math, and Matplotlib for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c0b4b",
   "metadata": {},
   "source": [
    "## Explore the Environment\n",
    "\n",
    "Create `CartPole-v1`, inspect action/observation spaces, and test a random policy for intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f97124",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=None)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "\n",
    "# quick random rollout (no render for speed)\n",
    "state, _ = env.reset()\n",
    "ret = 0\n",
    "for t in range(200):\n",
    "    a = env.action_space.sample()\n",
    "    state, r, term, trunc, _ = env.step(a)\n",
    "    ret += r\n",
    "    if term or trunc:\n",
    "        break\n",
    "print(f\"Random policy lasted {ret} steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b84b9",
   "metadata": {},
   "source": [
    "## Discretize the Continuous State\n",
    "\n",
    "CartPole's state is continuous. We'll bin each dimension into discrete buckets so a tabular Q-table can be used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efda6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = (12, 12, 18, 18)  # [cart_pos, cart_vel, pole_angle, pole_vel]\n",
    "\n",
    "# helper to make evenly spaced bin edges (exclude endpoints)\n",
    "def make_bins(low, high, n):\n",
    "    return np.linspace(low, high, n + 1)[1:-1]\n",
    "\n",
    "cart_pos_bins = make_bins(-4.8, 4.8, NUM_BINS[0])\n",
    "cart_vel_bins = make_bins(-3.0, 3.0, NUM_BINS[1])\n",
    "pole_ang_bins = make_bins(-0.418, 0.418, NUM_BINS[2])\n",
    "pole_vel_bins = make_bins(-3.5, 3.5, NUM_BINS[3])\n",
    "\n",
    "# map continuous state -> discrete indices\n",
    "def discretize(s):\n",
    "    cp, cv, pa, pv = s\n",
    "    return (\n",
    "        np.digitize(cp, cart_pos_bins),\n",
    "        np.digitize(cv, cart_vel_bins),\n",
    "        np.digitize(pa, pole_ang_bins),\n",
    "        np.digitize(pv, pole_vel_bins),\n",
    "    )\n",
    "\n",
    "print(\"Discretization ready. Example:\")\n",
    "print(discretize(np.array([0.0, 0.1, 0.02, -0.2])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1464dce",
   "metadata": {},
   "source": [
    "## Q-Learning Hyperparameters\n",
    "\n",
    "We'll use ε-greedy exploration and a tabular Q-table with shape `(bins..., num_actions)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "episodes = 5000\n",
    "max_steps = 500\n",
    "\n",
    "q = np.zeros(NUM_BINS + (env.action_space.n,))\n",
    "rewards, eps_history, ep_lengths = [], [], []\n",
    "\n",
    "q.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585ef07e",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "At each step:\n",
    "1. Choose action via ε-greedy\n",
    "2. Step environment\n",
    "3. Update Q using: `Q[s,a] ← Q[s,a] + α(r + γ max_a' Q[s',a'] − Q[s,a])`\n",
    "4. Decay ε after each episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ffd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(s_disc, eps):\n",
    "    if np.random.rand() < eps:\n",
    "        return env.action_space.sample()\n",
    "    return int(np.argmax(q[s_disc]))\n",
    "\n",
    "for ep in range(1, episodes + 1):\n",
    "    s, _ = env.reset()\n",
    "    s_disc = discretize(s)\n",
    "    total = 0\n",
    "    steps = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        a = choose_action(s_disc, epsilon)\n",
    "        s2, r, term, trunc, _ = env.step(a)\n",
    "        s2_disc = discretize(s2)\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next = np.max(q[s2_disc])\n",
    "        q[s_disc + (a,)] += alpha * (r + gamma * best_next - q[s_disc + (a,)])\n",
    "\n",
    "        s_disc = s2_disc\n",
    "        total += r\n",
    "        steps += 1\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    # decay exploration\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    rewards.append(total)\n",
    "    eps_history.append(epsilon)\n",
    "    ep_lengths.append(steps)\n",
    "\n",
    "    if ep % 100 == 0:\n",
    "        print(f\"Episode {ep:4d} | avg_reward(last 100): {np.mean(rewards[-100:]):6.2f} | eps: {epsilon:5.3f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95570ff4",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "Plot reward moving average, epsilon decay, and episode lengths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 100\n",
    "ma = np.convolve(rewards, np.ones(window)/window, mode='valid') if len(rewards) >= window else rewards\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16,4))\n",
    "axs[0].plot(rewards, alpha=0.3); axs[0].plot(range(len(ma)), ma, lw=2)\n",
    "axs[0].set_title('Episode Reward & Moving Average')\n",
    "axs[0].set_xlabel('Episode'); axs[0].set_ylabel('Reward')\n",
    "\n",
    "axs[1].plot(eps_history)\n",
    "axs[1].set_title('Epsilon Decay'); axs[1].set_xlabel('Episode'); axs[1].set_ylabel('Epsilon')\n",
    "\n",
    "axs[2].plot(ep_lengths, alpha=0.7)\n",
    "axs[2].set_title('Episode Lengths'); axs[2].set_xlabel('Episode'); axs[2].set_ylabel('Steps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e0f58e",
   "metadata": {},
   "source": [
    "## Visualize Trained Policy (Optional)\n",
    "\n",
    "Run a few episodes with the greedy policy (no exploration). Set `render_mode=\"human\"` to watch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_greedy(render=False, episodes=3):\n",
    "    mode = \"human\" if render else None\n",
    "    env_vis = gym.make(\"CartPole-v1\", render_mode=mode)\n",
    "    scores = []\n",
    "    for i in range(episodes):\n",
    "        s, _ = env_vis.reset()\n",
    "        s_disc = discretize(s)\n",
    "        total = 0\n",
    "        for t in range(500):\n",
    "            a = int(np.argmax(q[s_disc]))\n",
    "            s, r, term, trunc, _ = env_vis.step(a)\n",
    "            s_disc = discretize(s)\n",
    "            total += r\n",
    "            if term or trunc:\n",
    "                break\n",
    "            if render:\n",
    "                time.sleep(0.01)\n",
    "        scores.append(total)\n",
    "        if render:\n",
    "            time.sleep(0.2)\n",
    "    env_vis.close()\n",
    "    return scores\n",
    "\n",
    "scores = run_greedy(render=False, episodes=5)\n",
    "print(\"Greedy test scores:\", scores, \"| avg:\", np.mean(scores))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
