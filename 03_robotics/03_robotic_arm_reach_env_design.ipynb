{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Designing a Robotic Arm Reach Task (Custom Gym Env)\n",
        "\n",
        "In this tutorial:\n",
        "- Design a minimal custom environment API for a 2D arm reaching a target\n",
        "- Define observation/action spaces and reward shaping\n",
        "- Plug into SB3/PPO for baseline training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class ArmReachEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": []}\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.max_torque = 1.0\n",
        "        self.dt = 0.05\n",
        "        # state: [theta1, theta2, dtheta1, dtheta2, target_x, target_y]\n",
        "        high = np.array([np.pi, np.pi, 5.0, 5.0, 1.0, 1.0], dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=-self.max_torque, high=self.max_torque, shape=(2,), dtype=np.float32)\n",
        "        self.reset()\n",
        "    def reset(self, *, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.theta = np.random.uniform(-0.1, 0.1, size=2)\n",
        "        self.dtheta = np.zeros(2)\n",
        "        self.target = np.random.uniform(-0.8, 0.8, size=2)\n",
        "        obs = np.array([*self.theta, *self.dtheta, *self.target], dtype=np.float32)\n",
        "        return obs, {}\n",
        "    def step(self, action):\n",
        "        action = np.clip(action, -self.max_torque, self.max_torque)\n",
        "        self.dtheta += action * self.dt\n",
        "        self.theta += self.dtheta * self.dt\n",
        "        end_eff = self.forward_kinematics(self.theta)\n",
        "        dist = np.linalg.norm(end_eff - self.target)\n",
        "        reward = -dist - 0.01*np.sum(action**2)\n",
        "        terminated = dist < 0.05\n",
        "        truncated = False\n",
        "        obs = np.array([*self.theta, *self.dtheta, *self.target], dtype=np.float32)\n",
        "        return obs, reward, terminated, truncated, {}\n",
        "    @staticmethod\n",
        "    def forward_kinematics(theta):\n",
        "        l1 = l2 = 0.5\n",
        "        x = l1*np.cos(theta[0]) + l2*np.cos(theta[0]+theta[1])\n",
        "        y = l1*np.sin(theta[0]) + l2*np.sin(theta[0]+theta[1])\n",
        "        return np.array([x,y])\n",
        "\n",
        "env = ArmReachEnv()\n",
        "obs, _ = env.reset()\n",
        "print(\"Obs space:\", env.observation_space, \"Act space:\", env.action_space)\n",
        "for _ in range(5):\n",
        "    a = env.action_space.sample()\n",
        "    obs, r, term, trunc, _ = env.step(a)\n",
        "print(\"Sanity checked.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
