{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DQN from Scratch on CartPole\n",
        "\n",
        "In this tutorial, you will implement Deep Q-Networks (DQN) from scratch using PyTorch on `CartPole-v1`.\n",
        "\n",
        "Objectives:\n",
        "- Build a replay buffer and Q-network\n",
        "- Train with target network and ε-greedy exploration\n",
        "- Evaluate and visualize learning\n",
        "\n",
        "Read each markdown cell before running the corresponding code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Imports ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment and Seeding\n",
        "We'll use `CartPole-v1` for speed and clarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.n\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "obs_dim, act_dim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replay Buffer\n",
        "We store transitions `(s, a, r, s', done)` and sample random minibatches for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"s2\", \"d\"])\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=50_000):\n",
        "        self.buf = deque(maxlen=capacity)\n",
        "    def push(self, *args):\n",
        "        self.buf.append(Transition(*args))\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buf, batch_size)\n",
        "        s = torch.tensor(np.array([t.s for t in batch]), dtype=torch.float32)\n",
        "        a = torch.tensor([t.a for t in batch], dtype=torch.int64).unsqueeze(-1)\n",
        "        r = torch.tensor([t.r for t in batch], dtype=torch.float32).unsqueeze(-1)\n",
        "        s2 = torch.tensor(np.array([t.s2 for t in batch]), dtype=torch.float32)\n",
        "        d = torch.tensor([t.d for t in batch], dtype=torch.float32).unsqueeze(-1)\n",
        "        return s, a, r, s2, d\n",
        "    def __len__(self):\n",
        "        return len(self.buf)\n",
        "\n",
        "rb = ReplayBuffer()\n",
        "len(rb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q-Network and Target Network\n",
        "Two MLPs: one for online Q-values and one target network for stable targets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class QNet(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 128), nn.ReLU(),\n",
        "            nn.Linear(128, act_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "qnet = QNet(obs_dim, act_dim)\n",
        "qtarget = QNet(obs_dim, act_dim)\n",
        "qtarget.load_state_dict(qnet.state_dict())\n",
        "opt = optim.Adam(qnet.parameters(), lr=1e-3)\n",
        "\n",
        "qnet, qtarget\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n",
        "- Collect experience with ε-greedy\n",
        "- Sample batches and compute TD targets\n",
        "- Periodically sync target network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon = 1.0\n",
        "eps_min, eps_decay = 0.05, 0.995\n",
        "batch_size = 64\n",
        "gamma = 0.99\n",
        "target_update = 1000\n",
        "steps, episode, returns = 0, 0, []\n",
        "\n",
        "for episode in range(200):\n",
        "    s, _ = env.reset()\n",
        "    done = False\n",
        "    ret = 0\n",
        "    while not done:\n",
        "        steps += 1\n",
        "        if random.random() < epsilon:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                a = int(torch.argmax(qnet(torch.tensor(s, dtype=torch.float32).unsqueeze(0))).item())\n",
        "        s2, r, term, trunc, _ = env.step(a)\n",
        "        d = float(term or trunc)\n",
        "        rb.push(s, a, r, s2, d)\n",
        "        s = s2\n",
        "        ret += r\n",
        "\n",
        "        if len(rb) >= batch_size:\n",
        "            S, A, R, S2, D = rb.sample(batch_size)\n",
        "            q_vals = qnet(S).gather(1, A)\n",
        "            with torch.no_grad():\n",
        "                target = R + (1 - D) * gamma * qtarget(S2).max(1, keepdim=True)[0]\n",
        "            loss = nn.MSELoss()(q_vals, target)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "\n",
        "        if steps % target_update == 0:\n",
        "            qtarget.load_state_dict(qnet.state_dict())\n",
        "\n",
        "        if d == 1.0:\n",
        "            break\n",
        "\n",
        "    epsilon = max(eps_min, epsilon * eps_decay)\n",
        "    returns.append(ret)\n",
        "    if (episode+1) % 10 == 0:\n",
        "        print(f\"Ep {episode+1:3d} | Ret {np.mean(returns[-10:]):6.2f} | eps {epsilon:5.3f}\")\n",
        "\n",
        "plt.plot(returns); plt.title('DQN from Scratch - Returns'); plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
