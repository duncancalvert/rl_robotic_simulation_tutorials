{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31cb53f9",
   "metadata": {},
   "source": [
    "# Cartpole via Gymnasium\n",
    "\n",
    "\n",
    "Imagine a pole hinged to a cart that moves along a horizontal track. The goal of Cartpole is to keep the pole balanced upright by moving the cart left or right.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Media/cartpole.gif?raw=true\" alt=\"Lunar Lander\"/>\n",
    "</p>\n",
    "\n",
    "## Environment Dynamics (CartPole-v1)\n",
    "* State space (observations): A 4D vector:\n",
    "    * Cart position (x)\n",
    "    * Cart velocity (ẋ)\n",
    "    * Pole angle (θ)\n",
    "    * Pole angular velocity (θ̇)\n",
    "* Action space: Discrete actions:\n",
    "    * 0 = move cart to the left\n",
    "    * 1 = move cart to the right\n",
    "* Reward:\n",
    "    * +1 for every timestep the pole remains upright (i.e., the episode hasn't ended).\n",
    "* Episode ends when:\n",
    "    * Pole falls too far (angle > 12°)\n",
    "    * Cart moves too far from the center (position > ~2.4 units)\n",
    "    * Episode reaches 500 timesteps (in CartPole-v1)\n",
    "\n",
    "## Why is Cartpole Popular\n",
    "* Simple dynamics but requires real-time decision making.\n",
    "* Ideal for testing and debugging RL algorithms like Q-learning, DQN, PPO, etc.\n",
    "* Deterministic and quick to simulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a3efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085f87c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# NUM_BINS is the following states\n",
    "# [cart position, cart velocity, pole angle, pole angular velocity]\n",
    "\n",
    "NUM_BINS = (12, 12, 18, 18)\n",
    "\n",
    "# Tuned Q-learning hyperparameters for convergence\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.01\n",
    "episodes = 10000\n",
    "max_steps = 500\n",
    "\n",
    "# Binning function\n",
    "def create_bins(low, high, num_bins):\n",
    "    return np.linspace(low, high, num_bins + 1)[1:-1]\n",
    "\n",
    "cart_position_bins = create_bins(-4.8, 4.8, NUM_BINS[0])\n",
    "cart_velocity_bins = create_bins(-3.0, 3.0, NUM_BINS[1])\n",
    "pole_angle_bins = create_bins(-0.418, 0.418, NUM_BINS[2])\n",
    "pole_velocity_bins = create_bins(-3.5, 3.5, NUM_BINS[3])\n",
    "\n",
    "def discretize_state(state):\n",
    "    cart_pos, cart_vel, pole_angle, pole_vel = state\n",
    "    return (\n",
    "        np.digitize(cart_pos, cart_position_bins),\n",
    "        np.digitize(cart_vel, cart_velocity_bins),\n",
    "        np.digitize(pole_angle, pole_angle_bins),\n",
    "        np.digitize(pole_vel, pole_velocity_bins),\n",
    "    )\n",
    "\n",
    "# Initialize Q-table and logging\n",
    "q_table = np.zeros(NUM_BINS + (env.action_space.n,))\n",
    "reward_history = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    state_disc = discretize_state(state)\n",
    "    total_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state_disc])\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_state_disc = discretize_state(next_state)\n",
    "        total_reward += reward\n",
    "\n",
    "        best_next_action = np.max(q_table[next_state_disc])\n",
    "        q_table[state_disc + (action,)] += alpha * (reward + gamma * best_next_action - q_table[state_disc + (action,)])\n",
    "        state_disc = next_state_disc\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    reward_history.append(total_reward)\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1}: reward={total_reward:.0f}, epsilon={epsilon:.3f}\")\n",
    "        env_vis = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "        vis_state, _ = env_vis.reset()\n",
    "        vis_state_disc = discretize_state(vis_state)\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            vis_action = np.argmax(q_table[vis_state_disc])\n",
    "            vis_state, _, vis_terminated, vis_truncated, _ = env_vis.step(vis_action)\n",
    "            vis_state_disc = discretize_state(vis_state)\n",
    "            time.sleep(0.01)\n",
    "            if vis_terminated or vis_truncated:\n",
    "                break\n",
    "        time.sleep(0.02)\n",
    "        env_vis.close()\n",
    "\n",
    "env.close()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81278831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rewards and smoothed moving average\n",
    "window = 50\n",
    "rolling_avg = np.convolve(reward_history, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(reward_history, label=\"Reward per Episode\", alpha=0.3)\n",
    "plt.plot(range(window - 1, len(reward_history)), rolling_avg, label=f\"{window}-episode Moving Average\", linewidth=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Q-Learning Performance on CartPole-v1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b33a441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
